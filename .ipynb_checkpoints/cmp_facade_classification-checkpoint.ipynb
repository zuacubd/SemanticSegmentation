{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f000fec-bacd-471c-83d7-047332cf9dfb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1496c88-ea94-4fd2-8c97-4a292303cc9d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1748987572508,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "f1496c88-ea94-4fd2-8c97-4a292303cc9d",
    "outputId": "b8bb2e39-0cbc-4195-ab57-f68d84537505"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e1e523-4217-4310-bcef-f195d3ec5b72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c53170b-254a-4f50-ba76-128f609bee96",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1748987573954,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "4c53170b-254a-4f50-ba76-128f609bee96"
   },
   "outputs": [],
   "source": [
    "class FacadesDataset(Dataset):\n",
    "    def __init__(self, root_dir_A, transform=None):\n",
    "        self.root_dir_A = root_dir_A  # Directory for input images (label maps)\n",
    "        #self.root_dir_B = root_dir_B  # Directory for target images (facades)\n",
    "        self.transform = transform\n",
    "        # Get list of image paths from both directories\n",
    "        self.image_paths_A = sorted([os.path.join(root_dir_A, img) for img in os.listdir(root_dir_A) if img.endswith('.jpg')])\n",
    "        self.image_paths_B = sorted([os.path.join(root_dir_B, img) for img in os.listdir(root_dir_B) if img.endswith('.png')])\n",
    "\n",
    "        # Ensure the number of images match\n",
    "        assert len(self.image_paths_A) == len(self.image_paths_B), \"Mismatch in number of images between A and B directories\"\n",
    "        print(f\"Found {len(self.image_paths_A)} paired images in {root_dir_A} and {root_dir_B}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths_A)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load input (label map) and target (facade) images\n",
    "        input_image = Image.open(self.image_paths_A[idx]).convert('RGB')\n",
    "        target_image = Image.open(self.image_paths_B[idx]).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            target_image = self.transform(target_image)\n",
    "\n",
    "        return input_image, target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a848e02-3524-4bba-b8d3-431a8909b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacadesInferenceDataset(Dataset):\n",
    "    def __init__(self, root_dir_A, transform=None):\n",
    "        self.root_dir_A = root_dir_A  # Directory for input images (label maps)\n",
    "        #self.root_dir_B = root_dir_B  # Directory for target images (facades)\n",
    "        self.transform = transform\n",
    "        # Get list of image paths from both directories\n",
    "        self.image_paths_A = sorted([os.path.join(root_dir_A, img) for img in os.listdir(root_dir_A) if img.endswith('.jpg')])\n",
    "        #self.image_paths_B = sorted([os.path.join(root_dir_B, img) for img in os.listdir(root_dir_B) if img.endswith('.jpg')])\n",
    "\n",
    "        # Ensure the number of images match\n",
    "        #assert len(self.image_paths_A) == len(self.image_paths_B), \"Mismatch in number of images between A and B directories\"\n",
    "        print(f\"Found {len(self.image_paths_A)} in {root_dir_A}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths_A)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load input (label map) and target (facade) images\n",
    "        input_image = Image.open(self.image_paths_A[idx]).convert('RGB')\n",
    "        #target_image = Image.open(self.image_paths_B[idx]).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            #target_image = self.transform(target_image)\n",
    "\n",
    "        return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VJt7nbLZvTXu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1748987575558,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "VJt7nbLZvTXu",
    "outputId": "b7778a70-0507-4110-b1ef-c6b9865fb7ee"
   },
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "def get_transform():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "    ])\n",
    "    print(\"Transformations defined: Resize to 256x256, ToTensor, Normalize to [-1, 1]\")\n",
    "    return transform\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23558c5f-9d8c-4554-a9d9-0bda131497f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c639b7d8-cdfb-475c-b8d0-819d8db15348",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1748987577561,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "c639b7d8-cdfb-475c-b8d0-819d8db15348"
   },
   "outputs": [],
   "source": [
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.enc5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.enc6 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.enc7 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.enc8 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec7 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec8 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        e5 = self.enc5(e4)\n",
    "        e6 = self.enc6(e5)\n",
    "        e7 = self.enc7(e6)\n",
    "        e8 = self.enc8(e7)\n",
    "\n",
    "        d1 = self.dec1(e8)\n",
    "        d1 = torch.cat([d1, e7], dim=1)\n",
    "        d2 = self.dec2(d1)\n",
    "        d2 = torch.cat([d2, e6], dim=1)\n",
    "        d3 = self.dec3(d2)\n",
    "        d3 = torch.cat([d3, e5], dim=1)\n",
    "        d4 = self.dec4(d3)\n",
    "        d4 = torch.cat([d4, e4], dim=1)\n",
    "        d5 = self.dec5(d4)\n",
    "        d5 = torch.cat([d5, e3], dim=1)\n",
    "        d6 = self.dec6(d5)\n",
    "        d6 = torch.cat([d6, e2], dim=1)\n",
    "        d7 = self.dec7(d6)\n",
    "        d7 = torch.cat([d7, e1], dim=1)\n",
    "        d8 = self.dec8(d7)\n",
    "        return d8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c08471-db22-46a3-b618-6062b2892569",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1748987581506,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "50c08471-db22-46a3-b618-6062b2892569"
   },
   "outputs": [],
   "source": [
    "class PatchGANDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PatchGANDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(6, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        input = torch.cat([x, y], dim=1)  # Concatenate input and target/generated image\n",
    "        return self.model(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e5ab3-b698-4aec-8862-84d72a2ab782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c508cf-1c24-442a-8e99-f85e0f0a6ff4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d07a696-69d6-4867-b1ed-fac45ff1302d",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1748987584434,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "7d07a696-69d6-4867-b1ed-fac45ff1302d"
   },
   "outputs": [],
   "source": [
    "# Function to display images\n",
    "def show_images_two(input_img, target_img, title_first, title_second):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].imshow(input_img.permute(1, 2, 0) * 0.5 + 0.5)  # Denormalize\n",
    "    ax[0].set_title(title_first)\n",
    "    ax[1].imshow(target_img.permute(1, 2, 0) * 0.5 + 0.5)\n",
    "    ax[1].set_title(title_second)\n",
    "    plt.show()\n",
    "\n",
    "# Function to display images\n",
    "def show_images_three(input_img, generated_img, target_img, title_first, title_second, title_third):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "    ax[0].imshow(input_img.permute(1, 2, 0) * 0.5 + 0.5)  # Denormalize\n",
    "    ax[0].set_title(title_first)\n",
    "    ax[1].imshow(generated_img.permute(1, 2, 0) * 0.5 + 0.5)\n",
    "    ax[1].set_title(title_second)\n",
    "    ax[1].imshow(target_img.permute(1, 2, 0) * 0.5 + 0.5)\n",
    "    ax[1].set_title(title_third)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c7038-4737-41ef-8215-24f19f859c7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1748987623956,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "681c7038-4737-41ef-8215-24f19f859c7e",
    "outputId": "9902b9a1-f0cc-4960-8c5a-8ad5219488ad"
   },
   "outputs": [],
   "source": [
    "def generation_visualisation(input_img, generator):\n",
    "    # Test the generator with a sample input\n",
    "    with torch.no_grad():\n",
    "        sample_input = input_img.to(device)\n",
    "        sample_output = generator(sample_input)\n",
    "        print(f\"Generator test output shape: {sample_output.shape}\")\n",
    "\n",
    "    show_images(input_img.cpu().squeeze(), sample_output.cpu().squeeze(), \"Input Image\", \"Initial Generated Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AKqbdbeCxcZ2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3252,
     "status": "ok",
     "timestamp": 1748987598266,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "AKqbdbeCxcZ2",
    "outputId": "c8d44c74-6875-471a-8fd7-e2112ce5db7b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_samples(data_loader, number):\n",
    "    # Get and display a sample\n",
    "    for i in range (number):\n",
    "        input_img, target_img = next(iter(data_loader))\n",
    "        print(f\"\\nDisplaying sample {i+1} from the dataset:\")\n",
    "        show_images(input_img[0], target_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bqnUZmJ5wT9M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1140,
     "status": "ok",
     "timestamp": 1748987586857,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "bqnUZmJ5wT9M",
    "outputId": "a705c8fc-dcd0-4a83-af83-f079f154d674"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5650b17e-e9f5-4774-b3e8-870f6b867daa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Generator and Discriminator Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d9201-0eed-4164-b932-93a02501c97b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1748987618928,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "3a1d9201-0eed-4164-b932-93a02501c97b",
    "outputId": "8363f9ae-6189-482e-c4d5-ce7a50f43c9d"
   },
   "outputs": [],
   "source": [
    "# Initialize the generator\n",
    "def get_generator():\n",
    "    generator = UNetGenerator().to(device)\n",
    "    print(\"U-Net Generator defined and moved to GPU\")\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5fcbef-905d-4286-9d99-6d7f6126aade",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1748987619756,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "de5fcbef-905d-4286-9d99-6d7f6126aade",
    "outputId": "a09330d0-d16d-4d45-dee1-e72aadad82b3"
   },
   "outputs": [],
   "source": [
    "# Initialize the discriminator\n",
    "def get_discriminator():\n",
    "    discriminator = PatchGANDiscriminator().to(device)\n",
    "    print(\"PatchGAN Discriminator defined and moved to GPU\")\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f04d9-fbed-4417-b0ea-18fd16e7165a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1748987621098,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "4d4f04d9-fbed-4417-b0ea-18fd16e7165a",
    "outputId": "2b221230-68ad-4794-e409-4eb37e46187a"
   },
   "outputs": [],
   "source": [
    "def apply_weigts_init(generator, discriminator, weights_init):\n",
    "    generator.apply(weights_init)\n",
    "    discriminator.apply(weights_init)\n",
    "    print(\"Weights initialized with normal distribution (mean=0, std=0.02) for Conv layers and (mean=1, std=0.02) for BatchNorm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73c7e1-e2fd-4e23-a149-ec39bc80f7c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1748987621989,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "7b73c7e1-e2fd-4e23-a149-ec39bc80f7c1",
    "outputId": "d6a28f68-0fef-4b59-ab15-7efbba455482"
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "def get_BCEWithLogitsLoss():\n",
    "    criterion_GAN = nn.BCEWithLogitsLoss()\n",
    "    print(\"Loss functions defined: BCEWithLogitsLoss for GAN\")\n",
    "    return criterion_GAN\n",
    "    \n",
    "def get_L1Loss():\n",
    "    criterion_L1 = nn.L1Loss()\n",
    "    print(\"Loss functions defined: L1Loss for pixel-wise similarity\")\n",
    "    return criterion_L1\n",
    "\n",
    "def optimiser_G():\n",
    "    # Optimizer for generator\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    print(\"Optimizers defined: Adam with lr=0.0002, betas=(0.5, 0.999) for both Generator\")\n",
    "    return optimiser_G\n",
    "\n",
    "def optimiser_D():\n",
    "    # Optimiser for discriminator\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    print(\"Optimizers defined: Adam with lr=0.0002, betas=(0.5, 0.999) for Discriminator\")\n",
    "    return optimiser_D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e171e45-d0ae-4eb8-8625-45eab87a5cc5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Training Setup and procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e9af39-e7ab-48ab-b91e-c725af34c47b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1yAtdsFYrFUZ3AkulgQyBLe1m7Voyoysp"
    },
    "id": "a6e9af39-e7ab-48ab-b91e-c725af34c47b",
    "outputId": "27825832-d185-4629-c57c-2294b68d4f5b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_learning(num_epochs, lambda_L1, train_loader, generator, discriminator, optimiser_G, optimiser_D):\n",
    "\n",
    "    # Full training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (input_img, target_img) in enumerate(train_loader):\n",
    "            input_img = input_img.to(device)\n",
    "            target_img = target_img.to(device)\n",
    "    \n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            pred_real = discriminator(input_img, target_img)\n",
    "            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "    \n",
    "            generated_img = generator(input_img)\n",
    "            pred_fake = discriminator(input_img, generated_img.detach())\n",
    "            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "    \n",
    "            loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "    \n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            pred_fake = discriminator(input_img, generated_img)\n",
    "            loss_G_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "            loss_G_L1 = criterion_L1(generated_img, target_img) * lambda_L1\n",
    "            loss_G = loss_G_GAN + loss_G_L1\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Print progress every 100 steps\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch}/{num_epochs}], Step [{i}/{len(train_loader)}], \"\n",
    "                      f\"Loss_D: {loss_D.item():.4f}, Loss_G: {loss_G.item():.4f}, \"\n",
    "                      f\"Loss_G_GAN: {loss_G_GAN.item():.4f}, Loss_G_L1: {loss_G_L1.item():.4f}\")\n",
    "\n",
    "        #visualise after each epoch using the last batch\n",
    "        show_images_three(input_img.cpu().detach().squeeze(), generated_img.cpu().detach().squeeze(), target_img.cpu().detach().squeeze(), 'Input', 'Generated', Target')\n",
    "    print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51d637-bfe8-4d02-92ec-42b0529c4b60",
   "metadata": {
    "id": "3b51d637-bfe8-4d02-92ec-42b0529c4b60",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_testing(test_loader, generator):\n",
    "    generator.eval()\n",
    "\n",
    "    # Initialize accumulators for metrics\n",
    "    total_l1_loss = 0.0\n",
    "    total_psnr = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    # Disable gradient computation for testing\n",
    "    with torch.no_grad():\n",
    "        for i, (input_img, target_img) in enumerate(test_loader):\n",
    "            # Move images to the device (e.g., GPU)\n",
    "            input_img = input_img.to(device)\n",
    "            target_img = target_img.to(device)\n",
    "    \n",
    "            # Generate image using the pix2pix generator\n",
    "            generated_img = generator(input_img)\n",
    "    \n",
    "            # Compute L1 loss (on normalized images in [-1, 1])\n",
    "            l1_loss = criterion_L1(generated_img, target_img)\n",
    "            total_l1_loss += l1_loss.item()\n",
    "    \n",
    "            # Denormalize images to [0, 1] for PSNR calculation\n",
    "            generated_img_denorm = ((generated_img + 1) / 2).clamp(0, 1)\n",
    "            target_img_denorm = ((target_img + 1) / 2).clamp(0, 1)\n",
    "    \n",
    "            # Compute Mean Squared Error (MSE)\n",
    "            mse = torch.mean((generated_img_denorm - target_img_denorm) ** 2)\n",
    "    \n",
    "            # Compute PSNR\n",
    "            if mse > 0:\n",
    "                psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
    "                total_psnr += psnr.item()\n",
    "            else:\n",
    "                total_psnr += 100  # Assign a high PSNR if MSE is zero (perfect match)\n",
    "    \n",
    "            num_samples += 1\n",
    "    \n",
    "            # Visualize the first 5 examples safely\n",
    "            if i < 5:\n",
    "                input_img_denorm = ((input_img + 1) / 2).clamp(0, 1)\n",
    "                show_images_three(input_img_denorm.cpu().squeeze(0), generated_img_denorm.cpu().squeeze(0), target_img_denorm.cpu().squeeze(0), 'Input Label Map',  'Generated Facade', 'Real Facade')\n",
    "                #for a in ax:\n",
    "                #    a.axis('off')  # Hide axes for cleaner visuals\n",
    "                \n",
    "    # Compute average metrics\n",
    "    avg_l1_loss = total_l1_loss / num_samples\n",
    "    avg_psnr = total_psnr / num_samples\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Test Results:\")\n",
    "    print(f\"Average L1 Loss: {avg_l1_loss:.4f}\")\n",
    "    print(f\"Average PSNR: {avg_psnr:.2f} dB\")\n",
    "    return avg_l1_loss, avg_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a3d308-3dfe-4420-a696-0ae99c7df454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_inference(inference_loader, generator):\n",
    "    generator.eval()\n",
    "    \n",
    "    # Disable gradient computation for testing\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (input_img) in enumerate(inference_loader):\n",
    "            # Move images to the device (e.g., GPU)\n",
    "            input_img = input_img.to(device)\n",
    "            #target_img = target_img.to(device)\n",
    "    \n",
    "            # Generate image using the pix2pix generator\n",
    "            generated_img = generator(input_img)\n",
    "    \n",
    "            # Denormalize images to [0, 1] for PSNR calculation\n",
    "            generated_img_denorm = ((generated_img + 1) / 2).clamp(0, 1)\n",
    "    \n",
    "            num_samples += 1\n",
    "    \n",
    "            # Visualize the first 5 examples safely\n",
    "            if i < 5:\n",
    "                input_img_denorm = ((input_img + 1) / 2).clamp(0, 1)\n",
    "                show_images_two(input_img_denorm.cpu().squeeze(0), generated_img_denorm.cpu().squeeze(0), 'Input Label Map', 'Generated Facade')  \n",
    "                #for a in ax:\n",
    "                #    a.axis('off')  # Hide axes for cleaner visuals\n",
    "                #plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c98bef-3857-490b-83b0-6e8c1ee20d41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Training data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mVbbrl0GwR6p",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1748987588204,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "mVbbrl0GwR6p"
   },
   "outputs": [],
   "source": [
    "#data_path = \"/content/drive/MyDrive/ColabNotebooks/Research/data/facade_dataset\"\n",
    "data_path = \"/home/zia/Documents/research/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99588d37-a67b-4f46-889a-fda07def2e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = data_path + '/cmp_facade_base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba643b8e-50ad-4e0c-903d-63032a30965a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1748987589967,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "ba643b8e-50ad-4e0c-903d-63032a30965a",
    "outputId": "a1d4b698-2784-4854-f4b1-8fa24812f689"
   },
   "outputs": [],
   "source": [
    "# Define the root directories for trainA and trainB (adjust based on Kaggle dataset path)\n",
    "train_root_dir_A = train_data_path + '/base'\n",
    "print(f\"Dataset root directories set to: {train_root_dir_A} for (inputs) and (targets)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a2a1e-ed81-4eb3-93d2-8672e3a44fec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1748987593047,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "008a2a1e-ed81-4eb3-93d2-8672e3a44fec",
    "outputId": "779f9f73-ea3f-4206-b6ab-98b4551c569a"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_dataset = FacadesDataset(root_dir_A=train_root_dir_A, transform=get_transform())\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "print(f\"Dataset loaded with {len(train_dataset)} samples, DataLoader created with batch size 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de91a27-957b-489e-9807-d391a30ddcde",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing Procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6bc17a-2d52-4717-b175-2fd99f02b95a",
   "metadata": {},
   "source": [
    "#### Testing data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edf640-494a-4112-b896-f98a00645d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test data path\n",
    "# Define test dataset directories (adjust paths if necessary)\n",
    "test_data_path = data_path + '/cmp_facade_extended'  # Label maps\n",
    "test_root_dir_A = test_data_path + '/extended'  # Label maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb5dc76-208f-4982-afb5-7ad14a4f540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_dataset = FacadesDataset(root_dir_A=test_root_dir_A, transform=get_transform())\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "print(f\"Test dataset loaded with {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4cfff-eeff-4fbf-9454-214d6ef32db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36267630-28f5-4717-b943-72685eac65ee",
   "metadata": {},
   "source": [
    "#### Storing the model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VDkfA5A8z4E7",
   "metadata": {
    "id": "VDkfA5A8z4E7"
   },
   "outputs": [],
   "source": [
    "torch.save(generator, data_path + \"/model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efadde4-ed81-4e22-a621-5866ca7a937d",
   "metadata": {},
   "source": [
    "### Inference Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598c76d7-9898-43b9-a955-7db488bd7619",
   "metadata": {},
   "source": [
    "#### data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c43d6-f0c0-4006-a5da-a615d81be45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inference dataset directories (adjust paths if necessary)\n",
    "inference_root_dir_A = data_path + '/testNapier'  # Label maps\n",
    "#test_root_dir_B = data_path + '/testB'  # Real facades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6423f661-4add-4ef1-a30e-f40836bc6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "inference_dataset = FacadesInferenceDataset(root_dir_A=inference_root_dir_A, transform=transform)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=1, shuffle=False)\n",
    "print(f\"Test dataset loaded with {len(inference_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e071ed3-c349-4174-af34-4b979af5cc30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
