{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1496c88-ea94-4fd2-8c97-4a292303cc9d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1748987572508,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "f1496c88-ea94-4fd2-8c97-4a292303cc9d",
    "outputId": "b8bb2e39-0cbc-4195-ab57-f68d84537505"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c53170b-254a-4f50-ba76-128f609bee96",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1748987573954,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "4c53170b-254a-4f50-ba76-128f609bee96"
   },
   "outputs": [],
   "source": [
    "class FacadesDataset(Dataset):\n",
    "    def __init__(self, root_dir_A, root_dir_B, transform=None):\n",
    "        self.root_dir_A = root_dir_A  # Directory for input images (label maps)\n",
    "        self.root_dir_B = root_dir_B  # Directory for target images (facades)\n",
    "        self.transform = transform\n",
    "        # Get list of image paths from both directories\n",
    "        self.image_paths_A = sorted([os.path.join(root_dir_A, img) for img in os.listdir(root_dir_A) if img.endswith('.jpg')])\n",
    "        self.image_paths_B = sorted([os.path.join(root_dir_B, img) for img in os.listdir(root_dir_B) if img.endswith('.jpg')])\n",
    "\n",
    "        # Ensure the number of images match\n",
    "        assert len(self.image_paths_A) == len(self.image_paths_B), \"Mismatch in number of images between A and B directories\"\n",
    "        print(f\"Found {len(self.image_paths_A)} paired images in {root_dir_A} and {root_dir_B}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths_A)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load input (label map) and target (facade) images\n",
    "        input_image = Image.open(self.image_paths_A[idx]).convert('RGB')\n",
    "        target_image = Image.open(self.image_paths_B[idx]).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            target_image = self.transform(target_image)\n",
    "\n",
    "        return input_image, target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a848e02-3524-4bba-b8d3-431a8909b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacadesInferenceDataset(Dataset):\n",
    "    def __init__(self, root_dir_A, transform=None):\n",
    "        self.root_dir_A = root_dir_A  # Directory for input images (label maps)\n",
    "        #self.root_dir_B = root_dir_B  # Directory for target images (facades)\n",
    "        self.transform = transform\n",
    "        # Get list of image paths from both directories\n",
    "        self.image_paths_A = sorted([os.path.join(root_dir_A, img) for img in os.listdir(root_dir_A) if img.endswith('.jpg')])\n",
    "        #self.image_paths_B = sorted([os.path.join(root_dir_B, img) for img in os.listdir(root_dir_B) if img.endswith('.jpg')])\n",
    "\n",
    "        # Ensure the number of images match\n",
    "        #assert len(self.image_paths_A) == len(self.image_paths_B), \"Mismatch in number of images between A and B directories\"\n",
    "        print(f\"Found {len(self.image_paths_A)} in {root_dir_A}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths_A)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load input (label map) and target (facade) images\n",
    "        input_image = Image.open(self.image_paths_A[idx]).convert('RGB')\n",
    "        #target_image = Image.open(self.image_paths_B[idx]).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            #target_image = self.transform(target_image)\n",
    "\n",
    "        return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VJt7nbLZvTXu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1748987575558,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "VJt7nbLZvTXu",
    "outputId": "b7778a70-0507-4110-b1ef-c6b9865fb7ee"
   },
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "])\n",
    "print(\"Transformations defined: Resize to 256x256, ToTensor, Normalize to [-1, 1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c639b7d8-cdfb-475c-b8d0-819d8db15348",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1748987577561,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "c639b7d8-cdfb-475c-b8d0-819d8db15348"
   },
   "outputs": [],
   "source": [
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.enc5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.enc6 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.enc7 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.enc8 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec7 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec8 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        e5 = self.enc5(e4)\n",
    "        e6 = self.enc6(e5)\n",
    "        e7 = self.enc7(e6)\n",
    "        e8 = self.enc8(e7)\n",
    "\n",
    "        d1 = self.dec1(e8)\n",
    "        d1 = torch.cat([d1, e7], dim=1)\n",
    "        d2 = self.dec2(d1)\n",
    "        d2 = torch.cat([d2, e6], dim=1)\n",
    "        d3 = self.dec3(d2)\n",
    "        d3 = torch.cat([d3, e5], dim=1)\n",
    "        d4 = self.dec4(d3)\n",
    "        d4 = torch.cat([d4, e4], dim=1)\n",
    "        d5 = self.dec5(d4)\n",
    "        d5 = torch.cat([d5, e3], dim=1)\n",
    "        d6 = self.dec6(d5)\n",
    "        d6 = torch.cat([d6, e2], dim=1)\n",
    "        d7 = self.dec7(d6)\n",
    "        d7 = torch.cat([d7, e1], dim=1)\n",
    "        d8 = self.dec8(d7)\n",
    "        return d8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c08471-db22-46a3-b618-6062b2892569",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1748987581506,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "50c08471-db22-46a3-b618-6062b2892569"
   },
   "outputs": [],
   "source": [
    "class PatchGANDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PatchGANDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(6, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        input = torch.cat([x, y], dim=1)  # Concatenate input and target/generated image\n",
    "        return self.model(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b46c3-9b5f-48bf-a8c0-0ef09635b2a3",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1748987583406,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "0e5b46c3-9b5f-48bf-a8c0-0ef09635b2a3"
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d07a696-69d6-4867-b1ed-fac45ff1302d",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1748987584434,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "7d07a696-69d6-4867-b1ed-fac45ff1302d",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to display images\n",
    "def show_images(input_img, target_img):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].imshow(input_img.permute(1, 2, 0) * 0.5 + 0.5)  # Denormalize\n",
    "    ax[0].set_title('Input Image (Label Map)')\n",
    "    ax[1].imshow(target_img.permute(1, 2, 0) * 0.5 + 0.5)\n",
    "    ax[1].set_title('Target Image (Facade)')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bqnUZmJ5wT9M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1140,
     "status": "ok",
     "timestamp": 1748987586857,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "bqnUZmJ5wT9M",
    "outputId": "a705c8fc-dcd0-4a83-af83-f079f154d674"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mVbbrl0GwR6p",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1748987588204,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "mVbbrl0GwR6p"
   },
   "outputs": [],
   "source": [
    "#data_path = \"/content/drive/MyDrive/ColabNotebooks/Research/data/facade_dataset\"\n",
    "data_path = \"/home/zia/Documents/research/data/facade\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba643b8e-50ad-4e0c-903d-63032a30965a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1748987589967,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "ba643b8e-50ad-4e0c-903d-63032a30965a",
    "outputId": "a1d4b698-2784-4854-f4b1-8fa24812f689"
   },
   "outputs": [],
   "source": [
    "# Define the root directories for trainA and trainB (adjust based on Kaggle dataset path)\n",
    "root_dir_A = data_path + '/trainA'\n",
    "root_dir_B = data_path + '/trainB'\n",
    "print(f\"Dataset root directories set to: {root_dir_A} (inputs) and {root_dir_B} (targets)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a2a1e-ed81-4eb3-93d2-8672e3a44fec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1748987593047,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "008a2a1e-ed81-4eb3-93d2-8672e3a44fec",
    "outputId": "779f9f73-ea3f-4206-b6ab-98b4551c569a"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_dataset = FacadesDataset(root_dir_A=root_dir_A, root_dir_B = root_dir_B, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "print(f\"Dataset loaded with {len(train_dataset)} samples, DataLoader created with batch size 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AKqbdbeCxcZ2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3252,
     "status": "ok",
     "timestamp": 1748987598266,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "AKqbdbeCxcZ2",
    "outputId": "c8d44c74-6875-471a-8fd7-e2112ce5db7b"
   },
   "outputs": [],
   "source": [
    "# Get and display a sample\n",
    "for i in range (3):\n",
    "\n",
    "    input_img, target_img = next(iter(train_loader))\n",
    "    print(f\"\\nDisplaying sample {i+1} from the dataset:\")\n",
    "    show_images(input_img[0], target_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d9201-0eed-4164-b932-93a02501c97b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1748987618928,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "3a1d9201-0eed-4164-b932-93a02501c97b",
    "outputId": "8363f9ae-6189-482e-c4d5-ce7a50f43c9d"
   },
   "outputs": [],
   "source": [
    "# Initialize the generator\n",
    "generator = UNetGenerator().to(device)\n",
    "print(\"U-Net Generator defined and moved to GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5fcbef-905d-4286-9d99-6d7f6126aade",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1748987619756,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "de5fcbef-905d-4286-9d99-6d7f6126aade",
    "outputId": "a09330d0-d16d-4d45-dee1-e72aadad82b3"
   },
   "outputs": [],
   "source": [
    "# Initialize the discriminator\n",
    "discriminator = PatchGANDiscriminator().to(device)\n",
    "print(\"PatchGAN Discriminator defined and moved to GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f04d9-fbed-4417-b0ea-18fd16e7165a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1748987621098,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "4d4f04d9-fbed-4417-b0ea-18fd16e7165a",
    "outputId": "2b221230-68ad-4794-e409-4eb37e46187a"
   },
   "outputs": [],
   "source": [
    "generator.apply(weights_init)\n",
    "discriminator.apply(weights_init)\n",
    "print(\"Weights initialized with normal distribution (mean=0, std=0.02) for Conv layers and (mean=1, std=0.02) for BatchNorm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73c7e1-e2fd-4e23-a149-ec39bc80f7c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1748987621989,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "7b73c7e1-e2fd-4e23-a149-ec39bc80f7c1",
    "outputId": "d6a28f68-0fef-4b59-ab15-7efbba455482"
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "criterion_GAN = nn.BCEWithLogitsLoss()\n",
    "criterion_L1 = nn.L1Loss()\n",
    "print(\"Loss functions defined: BCEWithLogitsLoss for GAN, L1Loss for pixel-wise similarity\")\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "print(\"Optimizers defined: Adam with lr=0.0002, betas=(0.5, 0.999) for both Generator and Discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c7038-4737-41ef-8215-24f19f859c7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1748987623956,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "681c7038-4737-41ef-8215-24f19f859c7e",
    "outputId": "9902b9a1-f0cc-4960-8c5a-8ad5219488ad"
   },
   "outputs": [],
   "source": [
    "# Test the generator with a sample input\n",
    "with torch.no_grad():\n",
    "    sample_input = input_img.to(device)\n",
    "    sample_output = generator(sample_input)\n",
    "    print(f\"Generator test output shape: {sample_output.shape}\")\n",
    "\n",
    "# Visualize the initial output\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(sample_input.cpu().squeeze().permute(1, 2, 0) * 0.5 + 0.5)\n",
    "ax[0].set_title('Input Image')\n",
    "ax[1].imshow(sample_output.cpu().squeeze().permute(1, 2, 0) * 0.5 + 0.5)\n",
    "ax[1].set_title('Initial Generated Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5bdce8-d66a-4d87-9ee9-584d62e21c77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1x_bvI2ogyWHNnvQWpw7TiZ9NIwO5lnQx"
    },
    "executionInfo": {
     "elapsed": 320852,
     "status": "ok",
     "timestamp": 1748987946932,
     "user": {
      "displayName": "Md Zia Ullah",
      "userId": "03858716556199329935"
     },
     "user_tz": -60
    },
    "id": "5c5bdce8-d66a-4d87-9ee9-584d62e21c77",
    "outputId": "303af3a7-9edb-4c98-95e0-f810c5988709",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 200\n",
    "lambda_L1 = 100\n",
    "print(f\"Training setup: {num_epochs} epochs, L1 lambda = {lambda_L1}\")\n",
    "\n",
    "# Start with the first epoch to demonstrate\n",
    "for epoch in range(1):  # Just one epoch for now\n",
    "    for i, (input_img, target_img) in enumerate(train_loader):\n",
    "        input_img = input_img.to(device)\n",
    "        target_img = target_img.to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        pred_real = discriminator(input_img, target_img)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        generated_img = generator(input_img)\n",
    "        pred_fake = discriminator(input_img, generated_img.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        pred_fake = discriminator(input_img, generated_img)\n",
    "        loss_G_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "        loss_G_L1 = criterion_L1(generated_img, target_img) * lambda_L1\n",
    "        loss_G = loss_G_GAN + loss_G_L1\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Print progress every 100 steps\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch 0, Step {i}/{len(train_loader)}, \"\n",
    "                  f\"Loss_D: {loss_D.item():.4f}, Loss_G: {loss_G.item():.4f}, \"\n",
    "                  f\"Loss_G_GAN: {loss_G_GAN.item():.4f}, Loss_G_L1: {loss_G_L1.item():.4f}\")\n",
    "\n",
    "            # Visualize\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "            ax[0].imshow(input_img.cpu().detach().squeeze().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "            ax[0].set_title('Input')\n",
    "\n",
    "            ax[1].imshow(generated_img.cpu().detach().squeeze().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "            ax[1].set_title('Generated')\n",
    "\n",
    "            ax[2].imshow(target_img.cpu().detach().squeeze().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "            ax[2].set_title('Target')\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e9af39-e7ab-48ab-b91e-c725af34c47b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1yAtdsFYrFUZ3AkulgQyBLe1m7Voyoysp"
    },
    "id": "a6e9af39-e7ab-48ab-b91e-c725af34c47b",
    "outputId": "27825832-d185-4629-c57c-2294b68d4f5b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Full training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (input_img, target_img) in enumerate(train_loader):\n",
    "        input_img = input_img.to(device)\n",
    "        target_img = target_img.to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        pred_real = discriminator(input_img, target_img)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        generated_img = generator(input_img)\n",
    "        pred_fake = discriminator(input_img, generated_img.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        pred_fake = discriminator(input_img, generated_img)\n",
    "        loss_G_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "        loss_G_L1 = criterion_L1(generated_img, target_img) * lambda_L1\n",
    "        loss_G = loss_G_GAN + loss_G_L1\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Print progress every 100 steps\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Step [{i}/{len(train_loader)}], \"\n",
    "                  f\"Loss_D: {loss_D.item():.4f}, Loss_G: {loss_G.item():.4f}, \"\n",
    "                  f\"Loss_G_GAN: {loss_G_GAN.item():.4f}, Loss_G_L1: {loss_G_L1.item():.4f}\")\n",
    "\n",
    "    # Visualization after each epoch (using the last batch in the epoch)\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    ax[0].imshow(input_img.cpu().detach().squeeze().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "    ax[0].set_title('Input')\n",
    "\n",
    "    ax[1].imshow(generated_img.cpu().detach().squeeze().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "    ax[1].set_title('Generated')\n",
    "\n",
    "    ax[2].imshow(target_img.cpu().detach().squeeze().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "    ax[2].set_title('Target')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51d637-bfe8-4d02-92ec-42b0529c4b60",
   "metadata": {
    "id": "3b51d637-bfe8-4d02-92ec-42b0529c4b60",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "\n",
    "# Define test dataset directories (adjust paths if necessary)\n",
    "test_root_dir_A = data_path + '/testA'  # Label maps\n",
    "test_root_dir_B = data_path + '/testB'  # Real facades\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = FacadesDataset(root_dir_A=test_root_dir_A, root_dir_B=test_root_dir_B, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "print(f\"Test dataset loaded with {len(test_dataset)} samples\")\n",
    "\n",
    "# Initialize accumulators for metrics\n",
    "total_l1_loss = 0.0\n",
    "total_psnr = 0.0\n",
    "num_samples = 0\n",
    "\n",
    "# Disable gradient computation for testing\n",
    "with torch.no_grad():\n",
    "    for i, (input_img, target_img) in enumerate(test_loader):\n",
    "        # Move images to the device (e.g., GPU)\n",
    "        input_img = input_img.to(device)\n",
    "        target_img = target_img.to(device)\n",
    "\n",
    "        # Generate image using the pix2pix generator\n",
    "        generated_img = generator(input_img)\n",
    "\n",
    "        # Compute L1 loss (on normalized images in [-1, 1])\n",
    "        l1_loss = criterion_L1(generated_img, target_img)\n",
    "        total_l1_loss += l1_loss.item()\n",
    "\n",
    "        # Denormalize images to [0, 1] for PSNR calculation\n",
    "        generated_img_denorm = ((generated_img + 1) / 2).clamp(0, 1)\n",
    "        target_img_denorm = ((target_img + 1) / 2).clamp(0, 1)\n",
    "\n",
    "        # Compute Mean Squared Error (MSE)\n",
    "        mse = torch.mean((generated_img_denorm - target_img_denorm) ** 2)\n",
    "\n",
    "        # Compute PSNR\n",
    "        if mse > 0:\n",
    "            psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
    "            total_psnr += psnr.item()\n",
    "        else:\n",
    "            total_psnr += 100  # Assign a high PSNR if MSE is zero (perfect match)\n",
    "\n",
    "        num_samples += 1\n",
    "\n",
    "        # Visualize the first 5 examples safely\n",
    "        if i < 5:\n",
    "            input_img_denorm = ((input_img + 1) / 2).clamp(0, 1)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            ax[0].imshow(input_img_denorm.cpu().squeeze(0).permute(1, 2, 0).detach().numpy())\n",
    "            ax[0].set_title('Input Label Map')\n",
    "\n",
    "            ax[1].imshow(generated_img_denorm.cpu().squeeze(0).permute(1, 2, 0).detach().numpy())\n",
    "            ax[1].set_title('Generated Facade')\n",
    "\n",
    "            ax[2].imshow(target_img_denorm.cpu().squeeze(0).permute(1, 2, 0).detach().numpy())\n",
    "            ax[2].set_title('Real Facade')\n",
    "\n",
    "            for a in ax:\n",
    "                a.axis('off')  # Hide axes for cleaner visuals\n",
    "            plt.show()\n",
    "\n",
    "# Compute average metrics\n",
    "avg_l1_loss = total_l1_loss / num_samples\n",
    "avg_psnr = total_psnr / num_samples\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test Results:\")\n",
    "print(f\"Average L1 Loss: {avg_l1_loss:.4f}\")\n",
    "print(f\"Average PSNR: {avg_psnr:.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VDkfA5A8z4E7",
   "metadata": {
    "id": "VDkfA5A8z4E7"
   },
   "outputs": [],
   "source": [
    "torch.save(generator, data_path + \"/model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c43d6-f0c0-4006-a5da-a615d81be45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inference dataset directories (adjust paths if necessary)\n",
    "inference_root_dir_A = data_path + '/testNapier'  # Label maps\n",
    "#test_root_dir_B = data_path + '/testB'  # Real facades\n",
    "\n",
    "# Load test dataset\n",
    "inference_dataset = FacadesInferenceDataset(root_dir_A=inference_root_dir_A, transform=transform)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=1, shuffle=False)\n",
    "print(f\"Test dataset loaded with {len(inference_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a3d308-3dfe-4420-a696-0ae99c7df454",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "\n",
    "# Disable gradient computation for testing\n",
    "num_samples = 0\n",
    "with torch.no_grad():\n",
    "    for i, (input_img) in enumerate(inference_loader):\n",
    "        # Move images to the device (e.g., GPU)\n",
    "        input_img = input_img.to(device)\n",
    "        #target_img = target_img.to(device)\n",
    "\n",
    "        # Generate image using the pix2pix generator\n",
    "        generated_img = generator(input_img)\n",
    "\n",
    "        # Compute L1 loss (on normalized images in [-1, 1])\n",
    "        #l1_loss = criterion_L1(generated_img, target_img)\n",
    "        #total_l1_loss += l1_loss.item()\n",
    "\n",
    "        # Denormalize images to [0, 1] for PSNR calculation\n",
    "        generated_img_denorm = ((generated_img + 1) / 2).clamp(0, 1)\n",
    "        #target_img_denorm = ((target_img + 1) / 2).clamp(0, 1)\n",
    "\n",
    "        # Compute Mean Squared Error (MSE)\n",
    "        #mse = torch.mean((generated_img_denorm - target_img_denorm) ** 2)\n",
    "\n",
    "        # Compute PSNR\n",
    "        #if mse > 0:\n",
    "        #    psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
    "        #    total_psnr += psnr.item()\n",
    "        #else:\n",
    "        #    total_psnr += 100  # Assign a high PSNR if MSE is zero (perfect match)\n",
    "\n",
    "        num_samples += 1\n",
    "\n",
    "        # Visualize the first 5 examples safely\n",
    "        if i < 5:\n",
    "            input_img_denorm = ((input_img + 1) / 2).clamp(0, 1)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            ax[0].imshow(input_img_denorm.cpu().squeeze(0).permute(1, 2, 0).detach().numpy())\n",
    "            ax[0].set_title('Input Label Map')\n",
    "\n",
    "            ax[1].imshow(generated_img_denorm.cpu().squeeze(0).permute(1, 2, 0).detach().numpy())\n",
    "            ax[1].set_title('Generated Facade')\n",
    "\n",
    "            #ax[2].imshow(target_img_denorm.cpu().squeeze(0).permute(1, 2, 0).detach().numpy())\n",
    "            #ax[2].set_title('Real Facade')\n",
    "\n",
    "            for a in ax:\n",
    "                a.axis('off')  # Hide axes for cleaner visuals\n",
    "            plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
